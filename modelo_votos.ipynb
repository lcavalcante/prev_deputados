{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise TSE\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Configuration\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_set = pd.read_csv(\"eleicoes_2006_a_2010.csv\")\n",
    "complete_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train and test\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove not representative data\n",
    "complete_filtered = complete_set.drop(['sequencial_candidato', 'nome', 'uf', 'ocupacao', 'partido', 'estado_civil', 'cargo'], axis=1)\n",
    "\n",
    "\n",
    "#complete_filtered = complete_filtered.drop(['ano'], axis=1)\n",
    "#train_2006 = train_2006.drop(['ano'], axis=1)\n",
    "#test_2010 = test_2010.drop(['ano'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_set.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_filtered = complete_filtered.drop(['recursos_de_outros_candidatos/comites', 'recursos_de_pessoas_fisicas', 'recursos_de_pessoas_juridicas', 'recursos_proprios'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer\n",
    "------------\n",
    "Todas as colunas com valores nulos são relacionadas a receita, se fossem poucas linhas poderiamos apenas descartar esses valores, porém não é o caso. É uma decisão complicada mas como estamos seguindo esse pipeline https://www.kaggle.com/apapiu/regularized-linear-models \"Replace the numeric missing values (NaN's) with the mean of their respective columns\" aplicaremos a mesma técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2006 = complete_set.loc[lambda complete_set : complete_filtered['ano'] == 2006]\n",
    "test_2010 = complete_set.loc[lambda complete_set : complete_filtered['ano'] == 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade_doacoes\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votos = pd.DataFrame({\"log(votos + 1)\":np.log1p(train_2006[\"votos\"]), \"votos\":train_2006[\"votos\"],})\n",
    "votos.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = complete_filtered.dtypes[complete_set.dtypes != \"object\"].index\n",
    "\n",
    "print(numeric_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#log transform the target:\n",
    "train_2006[\"votos\"] = np.log1p(train_2006[\"votos\"])\n",
    "test_2010[\"votos\"] = np.log1p(test_2010[\"votos\"])\n",
    "\n",
    "\n",
    "skewed_feats = train_2006[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "complete_filtered[skewed_feats] = np.log1p(complete_filtered[skewed_feats])\n",
    "\n",
    "\n",
    "\n",
    "print(skewed_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_filtered = pd.get_dummies(complete_filtered)\n",
    "\n",
    "#complete_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling NA's with the mean of the column:\n",
    "#complete_filtered.dropna(inplace=True)\n",
    "#complete_filtered.shape\n",
    "complete_filtered = complete_filtered.fillna(complete_filtered.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2006_pre = complete_filtered.loc[lambda complete_filtered : complete_filtered['ano'] == 2006]\n",
    "test_2010_pre = complete_filtered.loc[lambda complete_filtered : complete_filtered['ano'] == 2010]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_2006_pre = train_2006_pre.drop(['ano'], axis=1)\n",
    "#train_2010_pre = train_2010_pre.drop(['ano'], axis=1)\n",
    "\n",
    "train_2006_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2006_pre.votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = pd.DataFrame({\"log(votos + 1)\":np.log1p(complete_filtered[\"votos\"]), \"votos\":complete_filtered[\"votos\"],})\n",
    "\n",
    "\n",
    "X_intermediate, X_test, y_intermediate, y_test = train_test_split(complete_filtered, \n",
    "                                                                  complete_filtered.votos, \n",
    "                                                                  shuffle=True,\n",
    "                                                                  test_size=0.2, \n",
    "                                                                  random_state=15)\n",
    "\n",
    "# train/validation split (gives us train and validation sets)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n",
    "                                                                y_intermediate,\n",
    "                                                                shuffle=False,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=2018)\n",
    "\n",
    "#X_train.drop(['votos', 'ano'], axis=1)\n",
    "#X_train.drop(['votos', 'ano'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_validation_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
    "    model.fit(X_train, y_train)\n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    validation_error = calc_validation_error(X_test, y_test, model)\n",
    "    return train_error, validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_2006_pre\n",
    "y_train = train_2006_pre.votos\n",
    "X_test = test_2010_pre\n",
    "y_test = test_2010_pre.votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "\n",
    "train_error, test_error = calc_metrics(X_train, y_train, X_test, y_test, lr)\n",
    "train_error, test_error = round(train_error, 3), round(test_error, 3)\n",
    "\n",
    "print('train error: {} | test error: {}'.format(train_error, test_error))\n",
    "print('train/test: {}'.format(round(test_error/train_error, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train error lower than test error.\n",
    "* Test error 30% worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isso é um indicador de que possivelmente a variância está alta e há overfitting, para diminuir a complexidade precisamos usar regularizacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votos_2006 = pd.DataFrame({\"votos\":train_2006_pre[\"votos\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_intermediate, X_test, y_intermediate, y_test = train_test_split(complete_filtered, \n",
    "#                                                                  votos, \n",
    "#                                                                  shuffle=True,\n",
    "#                                                                  test_size=0.2, \n",
    "#                                                                  random_state=15)\n",
    "\n",
    "# train/validation split (gives us train and validation sets)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_2006_pre,\n",
    "                                                                votos_2006,\n",
    "                                                                shuffle=False,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=2018)\n",
    "\n",
    "X_train.drop(['votos', 'ano'], axis=1)\n",
    "X_train.drop(['votos', 'ano'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape\n",
    "#X_validation.shape\n",
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0, 0.001, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1, 3, 5, 10, 30, 50, 100]\n",
    "#ridge_fitting = pd.Series()\n",
    "train_error = []\n",
    "test_error = []\n",
    "\n",
    "i = 0\n",
    "for alpha in alphas:\n",
    "    # instantiate and fit model\n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    # calculate errors\n",
    "    train_error.append(mean_squared_error(y_train, ridge.predict(X_train)))\n",
    "    #new_validation_error = mean_squared_error(y_validation, ridge.predict(X_validation))\n",
    "    test_error.append(mean_squared_error(y_test, ridge.predict(X_test)))\n",
    "    # print errors as report\n",
    "    print('alpha: {:7} | train error: {:5} | test error: {}'.\n",
    "          format(alpha,\n",
    "                 round(train_error[i],3),\n",
    "                 #round(new_validation_error,3),\n",
    "                 round(test_error[i],3)))\n",
    "    i += 1 \n",
    "\n",
    "#cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "#            for alpha in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_error))\n",
    "print(len(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(alphas, train_error)\n",
    "ax.plot(alphas, test_error)\n",
    "\n",
    "ax.set(xlabel='lambda', ylabel='train_error')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005], max_iter=100000).fit(X_train, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_cv(model_lasso).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the residuals as well:\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "preds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n",
    "preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n",
    "preds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete intermediate variables\n",
    "del X_intermediate, y_intermediate\n",
    "\n",
    "# print proportions\n",
    "print('train: {}% | validation: {}% | test {}%'.format(round(len(y_train)/len(votos),2),\n",
    "                                                       round(len(y_validation)/len(votos),2),\n",
    "                                                       round(len(y_test)/len(votos),2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
